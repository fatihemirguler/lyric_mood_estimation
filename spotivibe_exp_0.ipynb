{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39da382f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available:  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\melih\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\melih\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.trainer because of the following error (look up to see its traceback):\nFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\activations_tf.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1603\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1602\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_tf_utils.py:38\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollatorWithPadding, DefaultDataCollator\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations_tf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\activations_tf.py:27\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mmajor \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tf-keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m         )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gelu\u001b[39m(x):\n",
      "\u001b[1;31mValueError\u001b[0m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1603\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1602\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\integrations\\integration_utils.py:36\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel, TFPreTrainedModel\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m version\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1593\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1592\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1593\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1594\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1605\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1606\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1607\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1608\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1603\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1602\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:42\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Integrations must be imported before ML frameworks:\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     43\u001b[0m     get_reporting_integration_callbacks,\n\u001b[0;32m     44\u001b[0m     hp_params,\n\u001b[0;32m     45\u001b[0m )\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# isort: on\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1593\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1592\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1593\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1594\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1605\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1606\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1607\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1608\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, accuracy_score\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer,BertModel, BertForSequenceClassification, Trainer, TrainingArguments\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset, Dataset\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1593\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1591\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1592\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1593\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1594\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1605\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1606\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1607\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1608\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.trainer because of the following error (look up to see its traceback):\nFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "print(\"GPU Available: \", torch.cuda.is_available())\n",
    "import contractions\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import BertTokenizer,BertModel, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d18b84db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\melih\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\melih\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\melih\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"Annanay/aml_song_lyrics_balanced\")\n",
    "train_dataset = load_dataset(\"Annanay/aml_song_lyrics_balanced\", split=\"train\")\n",
    "train_df = train_dataset.to_pandas()\n",
    "\n",
    "test_dataset  = load_dataset(\"Annanay/aml_song_lyrics_balanced\", split=\"test\")\n",
    "test_df = test_dataset.to_pandas()\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a464d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a basic list of common English stopwords\n",
    "stop_words = { 'a', 'an', 'the', 'and', 'or', 'nor', 'so', 'yet', 'for', 'in', 'on', 'at', 'by',\n",
    "              'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\n",
    "              'above', 'below', 'to', 'from', 'up', 'down', 'out', 'over', 'under', 'again', 'further',\n",
    "              'is', 'am', 'are','was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'doing',\n",
    "              'will', 'would', 'shall', 'should', 'can', 'could', 'may', 'might', 'must', 'who', 'whom',\n",
    "              'whose', 'which', 'that', 'this', 'these', 'those', 'any', 'some', 'all', 'both',\n",
    "              'each', 'every', 'either', 'neither', 'another', 'such', 'up', 'down', 'out', 'off',\n",
    "              'over', 'under'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f80ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for text preprocessing\n",
    "def preprocess_lyrics(lyrics):\n",
    "    lyrics = contractions.fix(lyrics)\n",
    "    # Remove anything in brackets (like [Verse 1])\n",
    "    lyrics = re.sub(r'\\[.*?\\]', '', lyrics)\n",
    "    # Replace literal '\\n' with spaces\n",
    "    lyrics = lyrics.replace('\\\\n', ' ')\n",
    "    # Remove any isolated 'n' caused by incorrect newline handling\n",
    "    lyrics = re.sub(r'\\bn\\b', '', lyrics)\n",
    "    # Remove non-alphabetic characters\n",
    "    # lyrics = re.sub(r'[^a-zA-Z\\s]', '', lyrics)\n",
    "    # Convert to lowercase\n",
    "    lyrics = lyrics.lower()\n",
    "    # Tokenize the lyrics\n",
    "    tokens = word_tokenize(lyrics)\n",
    "    print()\n",
    "    # Remove stopwords and lemmatize the tokens\n",
    "    cleaned_lyrics = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(cleaned_lyrics)\n",
    "def minimal_cleaning(lyrics):\n",
    "    # Expand contractions (e.g., \"can't\" -> \"cannot\")\n",
    "    lyrics = contractions.fix(lyrics)\n",
    "    # Remove anything in brackets (like [Verse 1])\n",
    "    lyrics = re.sub(r'\\[.*?\\]', '', lyrics)\n",
    "    # Replace literal '\\n' with spaces\n",
    "    lyrics = lyrics.replace('\\\\n', ' ')\n",
    "    # Remove any isolated 'n' caused by incorrect newline handling\n",
    "    lyrics = re.sub(r'\\bn\\b', '', lyrics)\n",
    "    # Convert to lowercase (optional, depending on your model)\n",
    "    lyrics = lyrics.lower()\n",
    "    return lyrics\n",
    "\n",
    "def advanced_processing(lyrics):\n",
    "    # Tokenize the lyrics\n",
    "    tokens = word_tokenize(lyrics)\n",
    "    # Remove stopwords and lemmatize the tokens\n",
    "    cleaned_lyrics = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(cleaned_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe3f0c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_lyrics = [minimal_cleaning(lyric) for lyric in train_df['lyrics']]\n",
    "train_df['cleaned_lyrics'] = cleaned_lyrics\n",
    "\n",
    "cleaned_lyrics_test = [minimal_cleaning(lyric) for lyric in test_df['lyrics']]\n",
    "test_df['cleaned_lyrics'] = cleaned_lyrics_test\n",
    "\n",
    "# Encode the target variable (mood)\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['mood_encoded'] = label_encoder.fit_transform(train_df['mood'])\n",
    "test_df['mood_encoded'] = label_encoder.fit_transform(test_df['mood'])\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[['cleaned_lyrics', 'mood_encoded']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['cleaned_lyrics', 'mood_encoded']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12726a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load DistilBERT tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Tokenize dataset\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples['cleaned_lyrics'], padding='max_length', truncation=True, max_length=256)\n",
    "\n",
    "# train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "# test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'mood_encoded'])\n",
    "# test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'mood_encoded'])\n",
    "\n",
    "# train_dataset = train_dataset.rename_column(\"mood_encoded\", \"labels\")\n",
    "# test_dataset = test_dataset.rename_column(\"mood_encoded\", \"labels\")\n",
    "\n",
    "# # Load DistilBERT model and move to GPU\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_)).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13107c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',\n",
    "#     evaluation_strategy='epoch',\n",
    "#     save_strategy='epoch',\n",
    "#     learning_rate=2e-5,  \n",
    "#     per_device_train_batch_size=16,  \n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=2,  \n",
    "#     weight_decay=0.01,\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"eval_loss\",\n",
    "#     save_total_limit=1,\n",
    "#     logging_dir='./logs',\n",
    "#     logging_steps=10,\n",
    "#     report_to=\"tensorboard\",\n",
    "# )\n",
    "\n",
    "# # Trainer setup\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=test_dataset,\n",
    "# )\n",
    "\n",
    "# # Fine-tune the model\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "118025f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Evaluate the model\n",
    "# results = trainer.evaluate()\n",
    "\n",
    "# print(results)\n",
    "\n",
    "# # Get predictions\n",
    "# predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# # Extract predicted labels\n",
    "# preds = predictions.predictions.argmax(-1)\n",
    "\n",
    "# # Calculate accuracy, precision, recall, and F1-score\n",
    "# accuracy = accuracy_score(test_dataset['labels'], preds)\n",
    "# precision, recall, f1, _ = precision_recall_fscore_support(test_dataset['labels'], preds, average='weighted')\n",
    "\n",
    "# print(f'Test Accuracy: {accuracy:.4f}')\n",
    "# print(f'Test Precision: {precision:.4f}')\n",
    "# print(f'Test Recall: {recall:.4f}')\n",
    "# print(f'Test F1-score: {f1:.4f}')\n",
    "\n",
    "# # Get predictions\n",
    "# predictions = trainer.predict(train_dataset)\n",
    "\n",
    "# # Extract predicted labels\n",
    "# preds = predictions.predictions.argmax(-1)\n",
    "\n",
    "# # Calculate accuracy, precision, recall, and F1-score\n",
    "# accuracy = accuracy_score(train_dataset['labels'], preds)\n",
    "# precision, recall, f1, _ = precision_recall_fscore_support(train_dataset['labels'], preds, average='weighted')\n",
    "\n",
    "# print(f'Train Accuracy: {accuracy:.4f}')\n",
    "# print(f'Train Precision: {precision:.4f}')\n",
    "# print(f'Train Recall: {recall:.4f}')\n",
    "# print(f'Train F1-score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7f87ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('D:/Python_Projects/spotivibe_exp/data_with_cleaned_lyrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae9e8e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# import pandas as pd\n",
    "\n",
    "# # Create CountVectorizer for unigrams\n",
    "# unigram_vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "\n",
    "# # Fit the vectorizer on the training data (this keeps the matrix sparse)\n",
    "# X_train_unigram = unigram_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# # Convert to DataFrame but keep the sparse format\n",
    "# df_unigram = pd.DataFrame.sparse.from_spmatrix(X_train_unigram, columns=unigram_vectorizer.get_feature_names_out())\n",
    "\n",
    "# # Add labels to the DataFrame (y_train contains the mood categories)\n",
    "# df_unigram['label'] = y_train.values\n",
    "\n",
    "# # Function to count unique unigrams per label (optimized for sparse DataFrame)\n",
    "# def count_unique_ngrams_sparse(df):\n",
    "#     unique_ngrams_per_label = {}\n",
    "#     for label in df['label'].unique():\n",
    "#         # Select only rows belonging to the current label\n",
    "#         df_label = df[df['label'] == label]\n",
    "#         # Drop the label column to focus only on the features\n",
    "#         df_label = df_label.drop(columns=['label'])\n",
    "#         # Count non-zero (i.e., present) features for each label (optimized for sparse DataFrame)\n",
    "#         unique_ngrams_per_label[label] = df_label.astype(bool).sum(axis=1).sum()\n",
    "#     return unique_ngrams_per_label\n",
    "\n",
    "# # Count unique unigrams per label using sparse matrices\n",
    "# unigram_counts = count_unique_ngrams_sparse(df_unigram)\n",
    "\n",
    "# # Print the results\n",
    "# print(\"Unique Unigram Counts per Label:\", unigram_counts)\n",
    "# #OUTPUT: Unique Unigram Counts per Label: {1: 291334, 2: 244674, 0: 303024, 3: 319309}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f2591bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\melih\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "C:\\Users\\melih\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(data['cleaned_lyrics'], data['mood_cats'], test_size=0.2, random_state=42)\n",
    "# # Combine unigrams and bigrams in TF-IDF vectorizer\n",
    "# vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=768)\n",
    "# X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "# X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# # Load pre-trained BERT tokenizer and model, and move the model to the GPU\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# bert_model = BertModel.from_pretrained('bert-base-uncased').to('cuda')\n",
    "\n",
    "# # Function to extract BERT embeddings with CUDA\n",
    "# def extract_bert_embeddings(text_data):\n",
    "#     # Tokenize and encode the input text, and move inputs to the GPU\n",
    "#     inputs = tokenizer(text_data, return_tensors='pt', padding=True, truncation=True, max_length=512).to('cuda')\n",
    "    \n",
    "#     # Get the embeddings from the BERT model\n",
    "#     with torch.no_grad():\n",
    "#         outputs = bert_model(**inputs)\n",
    "    \n",
    "#     # Move the outputs back to the CPU (if necessary) and return the [CLS] token embedding\n",
    "#     return outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "# # Extract BERT embeddings for training and test sets, leveraging CUDA\n",
    "# X_train_bert = [extract_bert_embeddings(text) for text in X_train]\n",
    "# X_test_bert = [extract_bert_embeddings(text) for text in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6fc75b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11788, 768)\n",
      "(11788, 768)\n"
     ]
    }
   ],
   "source": [
    "# from scipy.sparse import hstack\n",
    "\n",
    "# # Convert TF-IDF features to dense format (to match BERT embeddings)\n",
    "# X_train_tfidf_dense = X_train_tfidf.toarray()\n",
    "# X_test_tfidf_dense = X_test_tfidf.toarray()\n",
    "\n",
    "# # Convert BERT embeddings to NumPy array\n",
    "# # Ensure we only extract the [CLS] token embedding for each input, so we have a 2D array\n",
    "# X_train_bert_cls = np.array([emb[0] for emb in X_train_bert])  # Extract the first embedding (CLS token)\n",
    "# X_test_bert_cls = np.array([emb[0] for emb in X_test_bert])\n",
    "\n",
    "# # Check the shapes to make sure both are 2D\n",
    "# print(X_train_tfidf_dense.shape)  # (number of samples, number of features)\n",
    "# print(X_train_bert_cls.shape)     # (number of samples, 768)\n",
    "\n",
    "# # Combine dense TF-IDF features with the CLS token BERT embeddings\n",
    "# X_train_combined = np.hstack([X_train_tfidf_dense, X_train_bert_cls])\n",
    "# X_test_combined = np.hstack([X_test_tfidf_dense, X_test_bert_cls])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8663a12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.54      0.53       770\n",
      "           1       0.50      0.47      0.49       819\n",
      "           2       0.64      0.61      0.62       671\n",
      "           3       0.81      0.84      0.83       687\n",
      "\n",
      "    accuracy                           0.61      2947\n",
      "   macro avg       0.61      0.62      0.62      2947\n",
      "weighted avg       0.61      0.61      0.61      2947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Train SVC model on the combined features\n",
    "# svc_model = SVC(kernel='rbf', C=10)\n",
    "# svc_model.fit(X_train_combined, y_train)\n",
    "\n",
    "# # Predict on the test set\n",
    "# y_pred_combined = svc_model.predict(X_test_combined)\n",
    "\n",
    "# # Evaluate the model\n",
    "# print(classification_report(y_test, y_pred_combined))\n",
    "# #  precision    recall  f1-score   support\n",
    "\n",
    "# #            0       0.52      0.55      0.53       770\n",
    "# #            1       0.51      0.48      0.50       819\n",
    "# #            2       0.66      0.61      0.64       671\n",
    "# #            3       0.81      0.85      0.83       687\n",
    "\n",
    "# #     accuracy                           0.62      2947\n",
    "# #    macro avg       0.62      0.62      0.62      2947\n",
    "# # weighted avg       0.61      0.62      0.62      2947"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2a4cbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\melih\\AppData\\Local\\Temp\\ipykernel_8904\\2022592209.py:40: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  X_train_bert = torch.tensor(X_train_bert).squeeze(1).to(device)\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from transformers import DistilBertTokenizer, DistilBertModel\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# # Check if CUDA is available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# # Load the tokenizer and the DistilBERT model\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# distilbert_model = DistilBertModel.from_pretrained('distilbert-base-uncased', output_hidden_states=True).to(device)\n",
    "\n",
    "# # Function to extract mean-pooled embeddings from DistilBERT\n",
    "# def extract_mean_pooled_embeddings(text_data):\n",
    "#     inputs = tokenizer(text_data, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = distilbert_model(**inputs)\n",
    "#     last_hidden_state = outputs.last_hidden_state\n",
    "#     # Mean pooling across all tokens\n",
    "#     mean_pooled_embedding = torch.mean(last_hidden_state, dim=1)\n",
    "#     return mean_pooled_embedding.cpu().numpy()\n",
    "\n",
    "# # Split the dataset (based on your data structure)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(data['cleaned_lyrics'], data['mood_cats'], test_size=0.2, random_state=42)\n",
    "\n",
    "# # One-hot encode the labels\n",
    "# # One-hot encode the labels\n",
    "# encoder = OneHotEncoder(sparse=False)\n",
    "# y_train_encoded = encoder.fit_transform(y_train.values.reshape(-1, 1))  # Convert Series to NumPy array\n",
    "# y_test_encoded = encoder.transform(y_test.values.reshape(-1, 1))  # Convert Series to NumPy array\n",
    "\n",
    "# # Extract BERT embeddings for the training and testing sets\n",
    "# X_train_bert = [extract_mean_pooled_embeddings(text) for text in X_train]\n",
    "# X_test_bert = [extract_mean_pooled_embeddings(text) for text in X_test]\n",
    "\n",
    "# # Convert to tensor and move to the appropriate device (GPU or CPU)\n",
    "# X_train_bert = torch.tensor(X_train_bert).squeeze(1).to(device)\n",
    "# y_train_tensor = torch.tensor(y_train_encoded).float().to(device)\n",
    "# X_test_bert = torch.tensor(X_test_bert).squeeze(1).to(device)\n",
    "# y_test_tensor = torch.tensor(y_test_encoded).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4554737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the feed-forward neural network model\n",
    "# class FFNN(nn.Module):\n",
    "#     def __init__(self, input_size, num_classes):\n",
    "#         super(FFNN, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, 128)  # First layer with 128 neurons\n",
    "#         self.fc2 = nn.Linear(128, 64)          # Second layer with 64 neurons\n",
    "#         self.fc3 = nn.Linear(64, num_classes)  # Output layer\n",
    "#         self.dropout = nn.Dropout(0.5)         # Dropout to prevent overfitting\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = torch.relu(self.fc2(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = torch.softmax(self.fc3(x), dim=1)\n",
    "#         return x\n",
    "    \n",
    "# # Instantiate the model and move it to the device (GPU or CPU)\n",
    "# input_size = 768  # DistilBERT embedding size is 768\n",
    "# num_classes = y_train_tensor.shape[1]  # Number of classes (from one-hot encoding)\n",
    "# model = FFNN(input_size=input_size, num_classes=num_classes).to(device)\n",
    "\n",
    "# # Define the loss function and optimizer\n",
    "# criterion = nn.BCELoss()  # For multi-class classification with one-hot encoded labels\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b63ae05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.5630422830581665\n",
      "Epoch 2/10, Loss: 0.562897801399231\n",
      "Epoch 3/10, Loss: 0.5626776218414307\n",
      "Epoch 4/10, Loss: 0.5625957250595093\n",
      "Epoch 5/10, Loss: 0.5620562434196472\n",
      "Epoch 6/10, Loss: 0.5617476105690002\n",
      "Epoch 7/10, Loss: 0.5615506768226624\n",
      "Epoch 8/10, Loss: 0.5615678429603577\n",
      "Epoch 9/10, Loss: 0.5613292455673218\n",
      "Epoch 10/10, Loss: 0.5610941648483276\n",
      "Test Accuracy: 30.23%\n"
     ]
    }
   ],
   "source": [
    "# # Training loop\n",
    "# epochs = 10\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     outputs = model(X_train_bert)\n",
    "#     loss = criterion(outputs, y_train_tensor)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "#     # Print training progress\n",
    "#     print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     outputs_test = model(X_test_bert)\n",
    "#     predictions = torch.argmax(outputs_test, dim=1)\n",
    "#     actuals = torch.argmax(y_test_tensor, dim=1)\n",
    "    \n",
    "#     accuracy = (predictions == actuals).float().mean().item()\n",
    "#     print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbdedfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 30.96%\n"
     ]
    }
   ],
   "source": [
    "# # Evaluate the model on the test set\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     outputs_test = model(X_train_bert)\n",
    "#     predictions = torch.argmax(outputs_test, dim=1)\n",
    "#     actuals = torch.argmax(y_train_tensor, dim=1)\n",
    "    \n",
    "#     accuracy = (predictions == actuals).float().mean().item()\n",
    "#     print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bec2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
